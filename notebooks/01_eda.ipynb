{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CICIDS2017 Dataset - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the CICIDS2017 Monday dataset (benign traffic baseline).\n",
    "\n",
    "**Objectives:**\n",
    "1. Load and inspect dataset structure\n",
    "2. Analyze feature distributions and statistics\n",
    "3. Identify data quality issues (missing values, inf values, duplicates)\n",
    "4. Explore feature correlations\n",
    "5. Visualize key patterns\n",
    "6. Document preprocessing requirements\n",
    "\n",
    "**Note:** If CICIDS2017 Monday data is not yet downloaded, this notebook includes sample data generation for development purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Attempt to load CICIDS2017 Monday dataset. If not available, generate sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "DATA_PATH = Path('../data/raw/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "\n",
    "def load_or_generate_data():\n",
    "    \"\"\"Load real data if available, otherwise generate sample for development.\"\"\"\n",
    "    \n",
    "    if DATA_PATH.exists():\n",
    "        print(f\"Loading CICIDS2017 Monday dataset from: {DATA_PATH}\")\n",
    "        print(\"This may take a minute for large files...\\n\")\n",
    "        \n",
    "        # Load with proper encoding handling\n",
    "        try:\n",
    "            df = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(DATA_PATH, encoding='latin1')\n",
    "        \n",
    "        # Clean column names (remove leading/trailing spaces)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        print(f\"Successfully loaded {len(df):,} rows\")\n",
    "        return df, True\n",
    "    \n",
    "    else:\n",
    "        print(\"CICIDS2017 data not found!\")\n",
    "        print(f\"Please download Monday-WorkingHours.pcap_ISCX.csv to: {DATA_PATH.absolute()}\")\n",
    "        print(\"See ../data/DOWNLOAD_INSTRUCTIONS.md for download steps.\\n\")\n",
    "        print(\"Generating sample data for development purposes...\\n\")\n",
    "        \n",
    "        # Generate sample data with realistic features\n",
    "        n_samples = 10000\n",
    "        \n",
    "        # Common CICIDS2017 features (subset for sample)\n",
    "        data = {\n",
    "            'Flow Duration': np.random.exponential(5000, n_samples),\n",
    "            'Total Fwd Packets': np.random.poisson(10, n_samples),\n",
    "            'Total Backward Packets': np.random.poisson(8, n_samples),\n",
    "            'Total Length of Fwd Packets': np.random.exponential(1000, n_samples),\n",
    "            'Total Length of Bwd Packets': np.random.exponential(800, n_samples),\n",
    "            'Fwd Packet Length Max': np.random.exponential(500, n_samples),\n",
    "            'Fwd Packet Length Min': np.random.exponential(50, n_samples),\n",
    "            'Fwd Packet Length Mean': np.random.exponential(200, n_samples),\n",
    "            'Fwd Packet Length Std': np.random.exponential(100, n_samples),\n",
    "            'Bwd Packet Length Max': np.random.exponential(450, n_samples),\n",
    "            'Bwd Packet Length Min': np.random.exponential(40, n_samples),\n",
    "            'Bwd Packet Length Mean': np.random.exponential(180, n_samples),\n",
    "            'Bwd Packet Length Std': np.random.exponential(90, n_samples),\n",
    "            'Flow Bytes/s': np.random.exponential(10000, n_samples),\n",
    "            'Flow Packets/s': np.random.exponential(100, n_samples),\n",
    "            'Flow IAT Mean': np.random.exponential(1000, n_samples),\n",
    "            'Flow IAT Std': np.random.exponential(500, n_samples),\n",
    "            'Flow IAT Max': np.random.exponential(2000, n_samples),\n",
    "            'Flow IAT Min': np.random.exponential(100, n_samples),\n",
    "            'Fwd IAT Total': np.random.exponential(5000, n_samples),\n",
    "            'Fwd IAT Mean': np.random.exponential(1000, n_samples),\n",
    "            'Fwd IAT Std': np.random.exponential(500, n_samples),\n",
    "            'Fwd IAT Max': np.random.exponential(2000, n_samples),\n",
    "            'Fwd IAT Min': np.random.exponential(100, n_samples),\n",
    "            'Bwd IAT Total': np.random.exponential(4500, n_samples),\n",
    "            'Bwd IAT Mean': np.random.exponential(900, n_samples),\n",
    "            'Bwd IAT Std': np.random.exponential(450, n_samples),\n",
    "            'Bwd IAT Max': np.random.exponential(1800, n_samples),\n",
    "            'Bwd IAT Min': np.random.exponential(90, n_samples),\n",
    "            'Fwd PSH Flags': np.random.poisson(1, n_samples),\n",
    "            'Bwd PSH Flags': np.random.poisson(1, n_samples),\n",
    "            'Fwd URG Flags': np.random.poisson(0.1, n_samples),\n",
    "            'Bwd URG Flags': np.random.poisson(0.1, n_samples),\n",
    "            'Fwd Header Length': np.random.poisson(40, n_samples),\n",
    "            'Bwd Header Length': np.random.poisson(35, n_samples),\n",
    "            'Fwd Packets/s': np.random.exponential(50, n_samples),\n",
    "            'Bwd Packets/s': np.random.exponential(40, n_samples),\n",
    "            'Min Packet Length': np.random.exponential(40, n_samples),\n",
    "            'Max Packet Length': np.random.exponential(500, n_samples),\n",
    "            'Packet Length Mean': np.random.exponential(200, n_samples),\n",
    "            'Packet Length Std': np.random.exponential(100, n_samples),\n",
    "            'Packet Length Variance': np.random.exponential(10000, n_samples),\n",
    "            'FIN Flag Count': np.random.poisson(1, n_samples),\n",
    "            'SYN Flag Count': np.random.poisson(1, n_samples),\n",
    "            'RST Flag Count': np.random.poisson(0.2, n_samples),\n",
    "            'PSH Flag Count': np.random.poisson(2, n_samples),\n",
    "            'ACK Flag Count': np.random.poisson(10, n_samples),\n",
    "            'URG Flag Count': np.random.poisson(0.1, n_samples),\n",
    "            'CWE Flag Count': np.random.poisson(0.1, n_samples),\n",
    "            'ECE Flag Count': np.random.poisson(0.1, n_samples),\n",
    "            'Down/Up Ratio': np.random.exponential(1, n_samples),\n",
    "            'Average Packet Size': np.random.exponential(200, n_samples),\n",
    "            'Avg Fwd Segment Size': np.random.exponential(180, n_samples),\n",
    "            'Avg Bwd Segment Size': np.random.exponential(160, n_samples),\n",
    "            'Fwd Header Length.1': np.random.poisson(40, n_samples),  # Duplicate column in real data\n",
    "            'Subflow Fwd Packets': np.random.poisson(10, n_samples),\n",
    "            'Subflow Fwd Bytes': np.random.exponential(1000, n_samples),\n",
    "            'Subflow Bwd Packets': np.random.poisson(8, n_samples),\n",
    "            'Subflow Bwd Bytes': np.random.exponential(800, n_samples),\n",
    "            'Init_Win_bytes_forward': np.random.exponential(8000, n_samples),\n",
    "            'Init_Win_bytes_backward': np.random.exponential(7000, n_samples),\n",
    "            'act_data_pkt_fwd': np.random.poisson(5, n_samples),\n",
    "            'min_seg_size_forward': np.random.exponential(20, n_samples),\n",
    "            'Active Mean': np.random.exponential(1000, n_samples),\n",
    "            'Active Std': np.random.exponential(500, n_samples),\n",
    "            'Active Max': np.random.exponential(2000, n_samples),\n",
    "            'Active Min': np.random.exponential(100, n_samples),\n",
    "            'Idle Mean': np.random.exponential(5000, n_samples),\n",
    "            'Idle Std': np.random.exponential(2500, n_samples),\n",
    "            'Idle Max': np.random.exponential(10000, n_samples),\n",
    "            'Idle Min': np.random.exponential(500, n_samples),\n",
    "            'Label': ['BENIGN'] * n_samples  # Monday data is all benign\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Introduce some realistic data quality issues\n",
    "        # Add some infinite values (division by zero in flow features)\n",
    "        inf_indices = np.random.choice(n_samples, size=50, replace=False)\n",
    "        df.loc[inf_indices, 'Flow Bytes/s'] = np.inf\n",
    "        df.loc[inf_indices[:25], 'Flow Packets/s'] = np.inf\n",
    "        \n",
    "        # Add some NaN values\n",
    "        nan_indices = np.random.choice(n_samples, size=30, replace=False)\n",
    "        df.loc[nan_indices, 'Fwd IAT Mean'] = np.nan\n",
    "        \n",
    "        # Add some duplicates\n",
    "        df = pd.concat([df, df.iloc[:100]], ignore_index=True)\n",
    "        \n",
    "        print(f\"Generated {len(df):,} sample rows with {len(df.columns)} features\")\n",
    "        print(\"Note: This is synthetic data for development only!\\n\")\n",
    "        return df, False\n",
    "\n",
    "# Load data\n",
    "df, is_real_data = load_or_generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Data Source: {'CICIDS2017 Real Data' if is_real_data else 'Sample/Synthetic Data'}\")\n",
    "print(f\"Number of Rows: {len(df):,}\")\n",
    "print(f\"Number of Columns: {len(df.columns)}\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and non-null counts\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY ISSUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate features and label\n",
    "label_col = 'Label'\n",
    "feature_cols = [col for col in df.columns if col != label_col]\n",
    "\n",
    "print(f\"\\nTotal Features: {len(feature_cols)}\")\n",
    "print(f\"Label Column: {label_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df[feature_cols].isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "missing_df_filtered = missing_df[missing_df['Missing Count'] > 0]\n",
    "\n",
    "print(f\"\\nColumns with Missing Values: {len(missing_df_filtered)}\")\n",
    "if len(missing_df_filtered) > 0:\n",
    "    print(missing_df_filtered.head(20))\n",
    "else:\n",
    "    print(\"No missing values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Infinite Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "inf_counts = {}\n",
    "for col in feature_cols:\n",
    "    if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "        inf_count = np.isinf(df[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            inf_counts[col] = inf_count\n",
    "\n",
    "print(f\"\\nColumns with Infinite Values: {len(inf_counts)}\")\n",
    "if inf_counts:\n",
    "    inf_df = pd.DataFrame.from_dict(inf_counts, orient='index', columns=['Inf Count'])\n",
    "    inf_df['Percentage'] = (inf_df['Inf Count'] / len(df)) * 100\n",
    "    print(inf_df.sort_values('Inf Count', ascending=False).head(20))\n",
    "else:\n",
    "    print(\"No infinite values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "duplicate_percent = (duplicate_count / len(df)) * 100\n",
    "\n",
    "print(f\"\\nDuplicate Rows: {duplicate_count:,} ({duplicate_percent:.2f}%)\")\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nNote: Duplicates will be removed during preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "print(\"\\nLabel Distribution:\")\n",
    "label_counts = df[label_col].value_counts()\n",
    "print(label_counts)\n",
    "print(f\"\\nUnique Labels: {df[label_col].nunique()}\")\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "label_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Monday data should be 100% BENIGN traffic (baseline for anomaly detection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric columns only\n",
    "numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric Features: {len(numeric_cols)}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics (first 10 features):\")\n",
    "df[numeric_cols[:10]].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for constant/near-constant features\n",
    "constant_features = []\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() == 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "print(f\"\\nConstant Features (zero variance): {len(constant_features)}\")\n",
    "if constant_features:\n",
    "    print(constant_features)\n",
    "    print(\"\\nNote: These should be removed during feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Distributions\n",
    "\n",
    "Visualize distributions of key features to understand data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features to visualize\n",
    "key_features = [\n",
    "    'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',\n",
    "    'Packet Length Mean', 'Average Packet Size'\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the dataset\n",
    "key_features = [f for f in key_features if f in df.columns]\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features[:8]):\n",
    "    # Remove inf and nan for visualization\n",
    "    data = df[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    axes[i].hist(data, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(feature, fontsize=10, fontweight='bold')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions (Key Features)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Many network features show heavy right-skew (exponential/power-law distributions)\")\n",
    "print(\"Preprocessing will apply log transformation or standardization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for subset of features\n",
    "subset_features = numeric_cols[:20]  # First 20 numeric features\n",
    "\n",
    "# Remove inf/nan for correlation calculation\n",
    "corr_data = df[subset_features].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            linewidths=0.5, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix (First 20 Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"\\nHighly Correlated Feature Pairs (|r| > 0.9): {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    print(\"\\nNote: Consider removing redundant features during feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'Total Rows': f\"{len(df):,}\",\n",
    "    'Total Features': len(feature_cols),\n",
    "    'Numeric Features': len(numeric_cols),\n",
    "    'Features with Missing Values': len(missing_df_filtered),\n",
    "    'Features with Infinite Values': len(inf_counts),\n",
    "    'Duplicate Rows': f\"{duplicate_count:,} ({duplicate_percent:.2f}%)\",\n",
    "    'Constant Features': len(constant_features),\n",
    "    'Highly Correlated Pairs': len(high_corr_pairs),\n",
    "    'Unique Labels': df[label_col].nunique(),\n",
    "    'Memory Usage (MB)': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:.<35} {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Requirements\n",
    "\n",
    "Based on the EDA findings, document required preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING REQUIREMENTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Required Preprocessing Steps:\")\n",
    "print()\n",
    "\n",
    "steps = [\n",
    "    (\"1. Remove Duplicates\", f\"Drop {duplicate_count:,} duplicate rows\"),\n",
    "    (\"2. Handle Infinite Values\", f\"Replace inf in {len(inf_counts)} columns with NaN or max/min values\"),\n",
    "    (\"3. Handle Missing Values\", f\"Impute or remove missing values in {len(missing_df_filtered)} columns\"),\n",
    "    (\"4. Remove Constant Features\", f\"Drop {len(constant_features)} zero-variance features\"),\n",
    "    (\"5. Handle Highly Correlated Features\", \"Consider removing redundant features (|r| > 0.95)\"),\n",
    "    (\"6. Feature Scaling\", \"Apply StandardScaler or MinMaxScaler to normalize feature ranges\"),\n",
    "    (\"7. Encode Labels\", \"Convert labels to binary (BENIGN=0 for anomaly detection)\"),\n",
    "    (\"8. Train/Test Split\", \"Split data maintaining temporal order if timestamps available\"),\n",
    "]\n",
    "\n",
    "for step, description in steps:\n",
    "    print(f\"{step}\")\n",
    "    print(f\"   â†’ {description}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "insights = [\n",
    "    \"Dataset Characteristics:\",\n",
    "    f\"  - {len(df):,} network flow records with {len(feature_cols)} features\",\n",
    "    f\"  - Monday data should be 100% benign traffic (baseline for anomaly detection)\",\n",
    "    f\"  - Memory footprint: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\",\n",
    "    \"\",\n",
    "    \"Data Quality Issues:\",\n",
    "    f\"  - Infinite values present in {len(inf_counts)} features (likely from division by zero)\",\n",
    "    f\"  - Missing values in {len(missing_df_filtered)} features\",\n",
    "    f\"  - {duplicate_count:,} duplicate rows need removal\",\n",
    "    f\"  - {len(constant_features)} constant features provide no information\",\n",
    "    \"\",\n",
    "    \"Feature Characteristics:\",\n",
    "    \"  - Most features show heavy right-skew (typical for network traffic)\",\n",
    "    \"  - Strong correlations between related features (e.g., packet counts and byte totals)\",\n",
    "    \"  - Wide range of scales (ms to MB/s) requires normalization\",\n",
    "    \"\",\n",
    "    \"Recommendations:\",\n",
    "    \"  - Use robust preprocessing pipeline to handle inf/nan values\",\n",
    "    \"  - Apply feature scaling (StandardScaler recommended)\",\n",
    "    \"  - Consider feature selection to reduce dimensionality\",\n",
    "    \"  - Monday (benign) data perfect for training unsupervised anomaly detectors\",\n",
    "    \"  - Use Tuesday-Friday data for testing anomaly detection performance\",\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "With EDA complete, proceed to:\n",
    "\n",
    "1. **Build Preprocessing Pipeline** (`src/data/preprocessing.py`)\n",
    "   - Implement cleaning functions\n",
    "   - Handle inf/nan values\n",
    "   - Feature scaling\n",
    "   - Train/test splitting\n",
    "\n",
    "2. **Implement Isolation Forest** (first anomaly detection algorithm)\n",
    "   - Train on Monday (benign) data\n",
    "   - Evaluate on test set\n",
    "   - Extract feature importance\n",
    "\n",
    "3. **Download Additional Days**\n",
    "   - Tuesday-Friday data for testing attack detection\n",
    "   - Evaluate per-attack-type performance\n",
    "\n",
    "4. **Update Progress Journal**\n",
    "   - Document EDA findings in `claude.md`\n",
    "   - Record preprocessing decisions\n",
    "   - Note any issues or blockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save EDA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save data quality report\n",
    "report = {\n",
    "    'total_rows': len(df),\n",
    "    'total_features': len(feature_cols),\n",
    "    'numeric_features': len(numeric_cols),\n",
    "    'missing_value_cols': len(missing_df_filtered),\n",
    "    'infinite_value_cols': len(inf_counts),\n",
    "    'duplicate_rows': int(duplicate_count),\n",
    "    'constant_features': len(constant_features),\n",
    "    'high_corr_pairs': len(high_corr_pairs),\n",
    "    'memory_mb': float(f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"),\n",
    "    'is_real_data': is_real_data\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(results_dir / 'eda_summary.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"EDA summary saved to: ../results/eda_summary.json\")\n",
    "print(\"\\nEDA Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
